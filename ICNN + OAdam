import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from IPython import display
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
from fairness import plot_distributions
from fairness import p_rule
from fairness import demographic_parity
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc
from optim import OAdam

torch.manual_seed(1)
np.random.seed(7)
sns.set(style="dark", palette="pastel", color_codes=True, context="notebook")
%matplotlib inline
from munch import Munch
import wandb
hyperpar = Munch(

    batch_size=32,
    test_size = 0.2,#test to train split
    num_epochs=200,#zerosumgame
    lr=0.001,#clf learning rate
    n_hidden1=512,#clf
    n_hidden2=128,#clf
    n_hidden3=64,#clf
    p_dropout = 0.2,#clf
    l2_lambda = 0.1,#clf l2 regularizer
    a_hidden1 = 32,#adv
    a_hidden2 = 16,#adv
    a_hidden3 = 16,#adv
    sens1weight = 50,
    sens2weight = 50
    
)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)

data = pd.read_csv('original_edit-2.csv')
data['first_generation_flag'].fillna(0, inplace=True)
data['first_generation_flag'] = data['first_generation_flag'].apply(lambda x: 1 if x >= 0.5 else 0).astype(int)
X = data.drop(['final_read_score','low_income','first_generation_flag'], axis=1)
y = data['final_read_score'].astype('int64')
Z = data[['low_income','first_generation_flag']]
n_features = X.shape[1]
n_sensitive = Z.shape[1]
# split into train/test set
(X_train, X_test, y_train, y_test, Z_train, Z_test) = train_test_split(X, y, Z, test_size=hyperpar.test_size, stratify=y, random_state=7)
# standardize the data
scaler = StandardScaler().fit(X_train)
scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), 
                                           columns=df.columns, index=df.index)
X_train = X_train.pipe(scale_df, scaler) 
X_test = X_test.pipe(scale_df, scaler) 
class PandasDataSet(TensorDataset):

    def __init__(self, *dataframes):
        tensors = (self._df_to_tensor(df) for df in dataframes)
        super(PandasDataSet, self).__init__(*tensors)

    def _df_to_tensor(self, df):
        if isinstance(df, pd.Series):
            df = df.to_frame('dummy')
        device_data = torch.from_numpy(df.values).float().to(device)
        
        return device_data

train_data = PandasDataSet(X_train, y_train, Z_train)
test_data = PandasDataSet(X_test, y_test, Z_test)
train_loader = DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False)
print('# training samples:', len(train_data))
print('# batches:', len(train_loader))
class ConvexNet(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        torch.manual_seed(2)
        layer_sizes = kwargs["size"] #[n_input, n_hidden_1, n_hidden_2, n_output]
        self.W = nn.ParameterList([nn.Parameter(torch.Tensor(l, layer_sizes[0]))
                                   for l in layer_sizes[1:]])
        self.U = nn.ParameterList([nn.Parameter(torch.Tensor(layer_sizes[i + 1], layer_sizes[i]))
                                   for i in range(1, len(layer_sizes) - 1)])
        self.bias = nn.ParameterList([nn.Parameter(torch.Tensor(l)) for l in layer_sizes[1:]])
        self.act = F.relu_
        self.use_dout = kwargs["use_dout"][0]
        self.dropout = nn.Dropout(p=kwargs["use_dout"][1])
        self.reset_parameters()
    def reset_parameters(self):
        # copying from PyTorch Linear
        for W in self.W:
            nn.init.kaiming_uniform_(W, a=5 ** 0.5)
        for U in self.U:
            nn.init.kaiming_uniform_(U, a=5 ** 0.5)
        for i, b in enumerate(self.bias):
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W[i])
            bound = 1 / (fan_in ** 0.5)
            nn.init.uniform_(b, -bound, bound)

    def forward(self, x):
        z = F.linear(x, self.W[0], self.bias[0])
        z = self.act(z)
        if self.use_dout:
            z = self.dropout(z)
        for W, b, U in zip(self.W[1:-1], self.bias[1:-1], self.U[:-1]):
            z = F.linear(x, W, b) + F.linear(z, F.softplus(U)) / U.shape[0]
            z = self.act(z)
            if self.use_dout:
                z = self.dropout(z)
        out = F.linear(x, self.W[-1], self.bias[-1]) + F.linear(z, F.softplus(self.U[-1])) / self.U[-1].shape[0]
        return torch.sigmoid(out)
n_features = n_features
n_hidden1 = hyperpar.n_hidden1
n_hidden2 = hyperpar.n_hidden2
n_hidden3 = hyperpar.n_hidden3
p_dropout = hyperpar.p_dropout
clf = ConvexNet(size = [n_features, n_hidden1, n_hidden2, n_hidden3, 1], use_dout = [True, p_dropout]) #working: 0.2 or 0.3
clf.to(device)
clf_criterion = nn.BCELoss()
clf_optimizer = OAdam(clf.parameters(), lr = hyperpar.lr)
def pretrain_classifier(clf, data_loader, optimizer, criterion):
    clf.to(device)
    for x, y, _ in data_loader:
        clf.zero_grad()
        p_y = clf(x)
        loss = criterion(p_y, y)
        l2_reg = 0.0
        for param in clf.parameters():
            l2_reg += torch.norm(param)**2    
        loss += 0.5 * l2_lambda * l2_reg
        loss.backward()
        optimizer.step()
    return clf
N_CLF_EPOCHS = 2
l2_lambda = hyperpar.l2_lambda  # Regularization parameter
for epoch in range(N_CLF_EPOCHS):
    clf = pretrain_classifier(clf, train_loader, clf_optimizer, clf_criterion)
class Adversary(nn.Module):
    def __init__(self, n_sensitive, a_hidden1=32, a_hidden2=32, a_hidden3=32):
        super(Adversary, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(1, a_hidden1),
            nn.ReLU(),
            nn.Linear(a_hidden1, a_hidden2),
            nn.ReLU(),
            nn.Linear(a_hidden2, a_hidden3),
            nn.ReLU(),
            nn.Linear(a_hidden3, n_sensitive),
        )
    def forward(self, x):
        return torch.sigmoid(self.network(x))
lambdas = torch.Tensor([hyperpar.sens1weight, hyperpar.sens2weight]).to(device)
a_hidden1 = hyperpar.a_hidden1
a_hidden2=hyperpar.a_hidden2
a_hidden3=hyperpar.a_hidden3
adv = ConvexNet(size = [1 ,a_hidden1, a_hidden2, a_hidden3, Z_train.shape[1]], 
                use_dout = [True, p_dropout]) #working: 0.2 or 0.3
adv.to(device)
adv_criterion = nn.BCELoss(reduce=False)
adv_optimizer = OAdam(adv.parameters(), lr = hyperpar.lr)
def pretrain_adversary(adv, clf, data_loader, optimizer, criterion):
    for x, _, z in data_loader:
        p_y = clf(x).detach()
        adv.zero_grad()
        p_z = adv(p_y)
        loss = (criterion(p_z, z) * lambdas).mean()
        loss.backward()
        optimizer.step()
    return adv

N_ADV_EPOCHS = 5
for epoch in range(N_ADV_EPOCHS):
    pretrain_adversary(adv, clf, train_loader, adv_optimizer, adv_criterion)
with torch.no_grad():
    pre_clf_test = clf(test_data.tensors[0])
    pre_adv_test = adv(pre_clf_test)
y_pre_clf = pd.Series(pre_clf_test.data.cpu().numpy().ravel(),
                      index=y_test.index)
y_pre_adv = pd.DataFrame(pre_adv_test.cpu().numpy(), columns=Z.columns)
fig = plot_distributions(y_test, Z_test, y_pre_clf, y_pre_adv)
fig.savefig('images/torch_biased_training.png')
epoch_metrics = {
    'p_rule_low_income': p_rule(y_pre_clf, Z_test['low_income']),
    'p_rule_first_generation': p_rule(y_pre_clf, Z_test['first_generation_flag']),
    'demographic_parity_low_income': demographic_parity(y_pre_clf, Z_test['low_income']),
    'demographic_parity_first_generation': demographic_parity(y_pre_clf, Z_test['first_generation_flag'])
    }

print("Epoch Metrics before Training:", epoch_metrics)
count = 0
def train(clf, adv, data_loader, clf_criterion, adv_criterion,
          clf_optimizer, adv_optimizer, lambdas):
    # Train adversary
    for x, y, z in data_loader:
        p_y = clf(x)
        adv.zero_grad()
        p_z = adv(p_y)
        loss_adv = (adv_criterion(p_z, z) * lambdas).mean()
        loss_adv.backward()
        adv_optimizer.step()
 
    # Train classifier on single batch
    for x, y, z in data_loader:
        pass
    p_y = clf(x)
    p_z = adv(p_y)
    clf.zero_grad()
    p_z = adv(p_y)
    loss_adv = (adv_criterion(p_z, z) * lambdas).mean()
    l2_reg = 0.0
    for param in clf.parameters():
            l2_reg += torch.norm(param)**2
    unfair_clf_loss = clf_criterion(p_y, y) + 0.5 * l2_lambda * l2_reg
    clf_loss = unfair_clf_loss - (adv_criterion(adv(p_y), z) * lambdas).mean()
    clf_loss.backward()
    clf_optimizer.step()
    if epoch%5 == 0:
      wandb.log(
                    {
                        "overall_fair_loss": clf_loss.item(),
                        "unfair_clf_loss": unfair_clf_loss.item()
#                         'l2_reg':l2_reg
                    },
                    step=epoch,
                )
    return clf, adv
N_EPOCH_COMBINED = hyperpar.num_epochs 
for epoch in range(1, N_EPOCH_COMBINED):
    
    clf, adv = train(clf, adv, train_loader, clf_criterion, adv_criterion,
                     clf_optimizer, adv_optimizer, lambdas)
    with torch.no_grad():
        clf_pred = clf(test_data.tensors[0])
        adv_pred = adv(clf_pred)
    y_post_clf = pd.Series(clf_pred.cpu().numpy().ravel(), index=y_test.index)
    Z_post_adv = pd.DataFrame(adv_pred.cpu().numpy(), columns=Z_test.columns)
    if epoch%10 == 0:
        fig = plot_distributions(y_test, Z_test, y_post_clf, Z_post_adv, epoch)
        display.clear_output(wait=True)
        plt.savefig(f'output/torch_{epoch+1:08d}.png', bbox_inches='tight')
        plt.show(plt.gcf())
    clf_accuracy = metrics.accuracy_score(y_test, y_post_clf > 0.5)
    if (clf_accuracy > count):
        save_clf = clf
        count = clf_accuracy
epoch_metrics = {
    'p_rule_low_income': p_rule(y_post_clf, Z_test['low_income']),
    'p_rule_first_generation': p_rule(y_post_clf, Z_test['first_generation_flag']),
    'demographic_parity_low_income': demographic_parity(y_post_clf, Z_test['low_income']),
    'demographic_parity_first_generation': demographic_parity(y_post_clf, Z_test['first_generation_flag'])
    }

print("Fairness Metrics after Training:")
print(epoch_metrics)  
last_img = f'output/torch_{epoch+1:08d}.png'
def test(clf, adv, test_loader):
    clf.eval()
    adv.eval()
    predictions = []
    adversary_predictions = []
    with torch.no_grad():
        for x, _, _ in test_loader:
            # Move data to device
            x = x.to(device)
            # Forward pass through the classifier and adversary
            clf_output = clf(x)
            adv_output = adv(clf_output)
            # Convert the predictions to CPU and numpy for further processing
            clf_pred = clf_output.cpu().numpy()
            adv_pred = adv_output.cpu().numpy()
            # Append predictions
            predictions.extend(clf_pred)
            adversary_predictions.extend(adv_pred)
    return predictions, adversary_predictions
# Call the test function
clf_predictions, adv_predictions = test(save_clf, adv, test_loader)
binary_predictions = [1 if pred > 0.5 else 0 for pred in clf_predictions]


clf_roc_auc = metrics.roc_auc_score(y_test, binary_predictions)
clf_accuracy = metrics.accuracy_score(y_test, binary_predictions)
clf_recall = metrics.recall_score(y_test, binary_predictions)
clf_precision = metrics.precision_score(y_test, binary_predictions)
clf_f1_score = metrics.f1_score(y_test, binary_predictions)
    
print('Classifier Performance Metrics')
print('Accuracy: {:.4f}'.format(clf_accuracy))
print('Precision: {:.4f}'.format(clf_precision))
print('Recall: {:.4f}'.format(clf_recall))
print('F1 Score: {:.4f}'.format(clf_f1_score))
print('AUROC Score: {:.4f}'.format(clf_roc_auc))

