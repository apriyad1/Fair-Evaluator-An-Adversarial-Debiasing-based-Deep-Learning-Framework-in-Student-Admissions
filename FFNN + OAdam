import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from IPython import display
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
from fairness.helpers import plot_distributions
from fairness.helpers import p_rule
from fairness.helpers import demographic_parity
from fairness.helpers import equalized_odds
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc


torch.manual_seed(1)
np.random.seed(7)
sns.set(style="dark", palette="pastel", color_codes=True, context="notebook")

%matplotlib inline
from munch import Munch
from optim import OAdam
import wandb
hyperpar = Munch(

    batch_size=64,
    test_size = 0.2,#test to train split
    num_epochs=180,#zerosumgame
    lr=0.001,#clf learning rate
    n_hidden1=256,#clf256
    n_hidden2=128,#clf
    n_hidden3 = 64,#clf
    n_hidden4=32,#clf32
    p_dropout = 0.2,#clf
    l2_lambda = 0.001,#clf l2 regularizer
    entropy_lambda = 0.0,#clf entropy regularizer
    a_hidden1 = 64,#adv32
    a_hidden2 = 16,#adv16
    a_hidden3 = 16,#adv16
    sens1weight = 50,
    sens2weight = 50
    
)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)
data = pd.read_csv('original_edit-2.csv')
data['first_generation_flag'].fillna(0, inplace=True)
data['first_generation_flag'] = data['first_generation_flag'].apply(lambda x: 1 if x >= 0.5 else 0).astype(int)
X = data.drop(['final_read_score','low_income','first_generation_flag'], axis=1)
y = data['final_read_score'].astype('int64')
Z = data[['low_income','first_generation_flag']]
n_features = X.shape[1]
n_sensitive = Z.shape[1]
# split into train/test set
(X_train, X_test, y_train, y_test, Z_train, Z_test) = train_test_split(X, y, Z, test_size=hyperpar.test_size, stratify=y, random_state=7)
# standardize the data
scaler = StandardScaler().fit(X_train)
scale_df = lambda df, scaler: pd.DataFrame(scaler.transform(df), 
                                           columns=df.columns, index=df.index)
X_train = X_train.pipe(scale_df, scaler) 
X_test = X_test.pipe(scale_df, scaler) 
class PandasDataSet(TensorDataset):

    def __init__(self, *dataframes):
        tensors = (self._df_to_tensor(df) for df in dataframes)
        super(PandasDataSet, self).__init__(*tensors)

    def _df_to_tensor(self, df):
        if isinstance(df, pd.Series):
            df = df.to_frame('dummy')
        device_data = torch.from_numpy(df.values).float().to(device)
        
        return device_data
        
train_data = PandasDataSet(X_train, y_train, Z_train)
test_data = PandasDataSet(X_test, y_test, Z_test)
train_loader = DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False)

print('# training samples:', len(train_data))
print('# batches:', len(train_loader))
class Classifier(nn.Module):

    def __init__(self, n_features, n_hidden1=32, n_hidden2=32, n_hidden3=32, n_hidden4=32, p_dropout=0.2):
        super(Classifier, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(n_features, n_hidden1),
            nn.ReLU(),
            nn.Dropout(p_dropout),
            nn.Linear(n_hidden1, n_hidden2),
            nn.ReLU(),
            nn.Dropout(p_dropout),
            nn.Linear(n_hidden2, n_hidden3),
#             nn.ReLU(),
            nn.Tanh(),
            nn.Dropout(p_dropout),
            nn.Linear(n_hidden3, 1)
            
#             nn.Linear(n_hidden3, n_hidden4),
#             nn.Tanh(),
#             nn.Dropout(p_dropout),
#             nn.Linear(n_hidden4, 1)
        )

    def forward(self, x):
        return torch.sigmoid(self.network(x))
clf = Classifier(n_features=n_features, n_hidden1 = hyperpar.n_hidden1, n_hidden2 = hyperpar.n_hidden2, 
                 n_hidden3 = hyperpar.n_hidden3, p_dropout = hyperpar.p_dropout)
clf.to(device)

clf_criterion = nn.BCELoss()

decay_rate = hyperpar.lr / hyperpar.num_epochs

clf_optimizer = OAdam(clf.parameters(), lr = hyperpar.lr)
def pretrain_classifier(clf, data_loader, optimizer, criterion):
    for x, y, _ in data_loader:
#         x = x.to(device)
#         y = y.to(device)
        clf.zero_grad()
        p_y = clf(x)
        loss = criterion(p_y, y)
        l2_reg = 0.0
        entr_reg = 0.0
        for param in clf.parameters():
            l2_reg += torch.norm(param)**2
            ####
            entr_reg += torch.mean(torch.sum(-p_y * torch.log(p_y), dim=1))
            ####
        
        loss += 0.5 * l2_lambda * l2_reg + entropy_lambda * entr_reg

        loss.backward()
        optimizer.step()
    return clf


N_CLF_EPOCHS = 2
l2_lambda = hyperpar.l2_lambda  # Regularization parameter
entropy_lambda = hyperpar.entropy_lambda

for epoch in range(N_CLF_EPOCHS):
    clf = pretrain_classifier(clf, train_loader, clf_optimizer, clf_criterion)
class Adversary(nn.Module):

    def __init__(self, n_sensitive, a_hidden1=32, a_hidden2=32, a_hidden3=32):
        super(Adversary, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(1, a_hidden1),
            nn.ReLU(),
            nn.Linear(a_hidden1, a_hidden2),
            nn.ReLU(),
            nn.Linear(a_hidden2, a_hidden3),
            nn.ReLU(),
            nn.Linear(a_hidden3, n_sensitive),
        )

    def forward(self, x):
        return torch.sigmoid(self.network(x))




#Fairness parameter
# lambdas = torch.Tensor([130, 30]).to(device)
lambdas = torch.Tensor([hyperpar.sens1weight, hyperpar.sens2weight]).to(device)


adv = Adversary(Z_train.shape[1], a_hidden1 = hyperpar.a_hidden1, a_hidden2=hyperpar.a_hidden2, 
                a_hidden3=hyperpar.a_hidden3)
adv.to(device)
adv_criterion = nn.BCELoss(reduce=False)
# adv_optimizer = optim.Adam(adv.parameters())

adv_optimizer = OAdam(adv.parameters(), lr = hyperpar.lr)
def pretrain_adversary(adv, clf, data_loader, optimizer, criterion):
    for x, _, z in data_loader:
        p_y = clf(x).detach()
        adv.zero_grad()
        p_z = adv(p_y)
        loss = (criterion(p_z, z) * lambdas).mean()
        loss.backward()
        optimizer.step()
    return adv

N_ADV_EPOCHS = 3

for epoch in range(N_ADV_EPOCHS):
    pretrain_adversary(adv, clf, train_loader, adv_optimizer, adv_criterion)
with torch.no_grad():
    pre_clf_test = clf(test_data.tensors[0])
    pre_adv_test = adv(pre_clf_test)


y_pre_clf = pd.Series(pre_clf_test.data.cpu().numpy().ravel(),
                      index=y_test.index)
y_pre_adv = pd.DataFrame(pre_adv_test.cpu().numpy(), columns=Z.columns)

# print("Z_test outside plot_distribution \n", Z_test)

fig = plot_distributions(y_test, Z_test, y_pre_clf, y_pre_adv)
fig.savefig('images/torch_biased_training.png')

epoch_metrics = {
    'p_rule_low_income': p_rule(y_pre_clf, Z_test['low_income']),
    'p_rule_first_generation': p_rule(y_pre_clf, Z_test['first_generation_flag']),
    'demographic_parity_low_income': demographic_parity(y_pre_clf, Z_test['low_income']),
    'demographic_parity_first_generation': demographic_parity(y_pre_clf, Z_test['first_generation_flag'])
    }

print("Fairness Metrics before Training:")
print(epoch_metrics)
count = 0
def train(clf, adv, data_loader, clf_criterion, adv_criterion,
          clf_optimizer, adv_optimizer, lambdas, epoch):
    
    # Train adversary
    for x, y, z in data_loader:
        p_y = clf(x)
        adv.zero_grad()
        p_z = adv(p_y)
        loss_adv = (adv_criterion(p_z, z) * lambdas).mean()
        loss_adv.backward()
        adv_optimizer.step()
 
    # Train classifier on single batch
    for x, y, z in data_loader:
        pass
    p_y = clf(x)
    p_z = adv(p_y)
    clf.zero_grad()
    p_z = adv(p_y)
    loss_adv = (adv_criterion(p_z, z) * lambdas).mean()
    l2_reg = 0.0
    entr_reg = 0.0
    for param in clf.parameters():
            l2_reg += torch.norm(param)**2
            entr_reg += torch.mean(torch.sum(-p_y * torch.log(p_y), dim=1))
    
    unfair_clf_loss = clf_criterion(p_y, y) + 0.5 * l2_lambda * l2_reg + entropy_lambda * entr_reg
    clf_loss = unfair_clf_loss - (adv_criterion(adv(p_y), z) * lambdas).mean()
    clf_loss.backward()
    clf_optimizer.step()
    
    if epoch%5 == 0:
        wandb.log(
                    {
                        "overall_fair_loss": clf_loss.item(),
                        "unfair_clf_loss": unfair_clf_loss.item(),
                        'l2_reg':l2_reg
                    },
                    step=epoch,
                )
return clf, adv

N_EPOCH_COMBINED = hyperpar.num_epochs 
for epoch in range(1, N_EPOCH_COMBINED):
    clf, adv = train(clf, adv, train_loader, clf_criterion, adv_criterion,
                     clf_optimizer, adv_optimizer, lambdas, epoch)
    with torch.no_grad():
        clf_pred = clf(test_data.tensors[0])
        adv_pred = adv(clf_pred)
    y_post_clf = pd.Series(clf_pred.cpu().numpy().ravel(), index=y_test.index)
    Z_post_adv = pd.DataFrame(adv_pred.cpu().numpy(), columns=Z_test.columns)
    
    clf_accuracy = metrics.accuracy_score(y_test, y_post_clf > 0.5)
    clf_recall = metrics.recall_score(y_test, y_post_clf > 0.5)
    if (clf_recall > count):
        save_clf = clf
        count = clf_recall
    if epoch%10 == 0:
        fig = plot_distributions(y_test, Z_test, y_post_clf, Z_post_adv, epoch)
        display.clear_output(wait=True)
        plt.savefig(f'output/torch_{epoch+1:08d}.png', bbox_inches='tight')
        plt.show(plt.gcf())
        
        
epoch_metrics = {
    'p_rule_low_income': p_rule(y_post_clf, Z_test['low_income']),
    'p_rule_first_generation': p_rule(y_post_clf, Z_test['first_generation_flag']),
    'demographic_parity_low_income': demographic_parity(y_post_clf, Z_test['low_income']),
    'demographic_parity_first_generation': demographic_parity(y_post_clf, Z_test['first_generation_flag'])
    }
print("Fairness Metrics after Training:")
print(epoch_metrics)
last_img = f'output/torch_{epoch+1:08d}.png'

def test(clf, adv, test_loader):
    clf.eval()
    adv.eval()
    predictions = []
    adversary_predictions = []
    with torch.no_grad():
        for x, _, _ in test_loader:
            # Move data to device
            x = x.to(device)

            # Forward pass through the classifier and adversary
            clf_output = clf(x)
            adv_output = adv(clf_output)

            # Convert the predictions to CPU and numpy for further processing
            clf_pred = clf_output.cpu().numpy()
            adv_pred = adv_output.cpu().numpy()

            # Append predictions
            predictions.extend(clf_pred)
            adversary_predictions.extend(adv_pred)

    return predictions, adversary_predictions

# Call the test function
clf_predictions, adv_predictions = test(save_clf, adv, test_loader)
from sklearn import metrics
binary_predictions = [1 if pred > 0.5 else 0 for pred in clf_predictions]
clf_roc_auc = metrics.roc_auc_score(y_test, binary_predictions)
clf_accuracy = metrics.accuracy_score(y_test, binary_predictions)
clf_recall = metrics.recall_score(y_test, binary_predictions)
clf_precision = metrics.precision_score(y_test, binary_predictions)
clf_f1_score = metrics.f1_score(y_test, binary_predictions)
    
print('Classifier Performance Metrics')
print('Accuracy: {:.4f}'.format(clf_accuracy))
print('Precision: {:.4f}'.format(clf_precision))
print('Recall: {:.4f}'.format(clf_recall))
print('F1 Score: {:.4f}'.format(clf_f1_score))
print('AUROC Score: {:.4f}'.format(clf_roc_auc))
